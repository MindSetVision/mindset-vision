# Decoder Approach

The decoder approach involves integrating simple networks, termed as _decoders_, at various stages of a pre-trained network. The primary network remains frozen while the decoders are trained on a new task, which can then be tested to investigate the attributes of the main network.

In our implementation, a ResNet152 pre-trained on ImageNet serves as the main network. We employ six decoders  positioned at different processing stages to cover a comprehensive range: from the input image to the output logits, with additional decoders situated at intermediary stages. The specific configuration is detailed in the [Decoder Placement section](#decoder-placement).

Two types of decoders are utilized:
1. A basic one consisting of a single fully connected linear layer.
2. A _residual decoder_, which is a deeper network featuring three pre-activation residual blocks, each containing two convolutional layers. 

While the _residual decoder_ approach enhance the performance of the decoder, there's a potential risk of overshadowing the capabilities of the main ResNet152. Initially, we recommend utilizing linear decoders, transitioning to the residual network only if the decoders exhibit learning deficiencies.

## Code Overview 
### Main Functions

The decoder approach generally entails training and evaluating the decoders. The necessary code is in the [`train.py`](train.py) and [`eval.py`](eval.py) modules, both utilizing the `annotation.csv` file generated by each dataset. They are designed to work both for `classification` and `regression` tasks. Training and evaluation sessions are configured using `toml` files, with each training session identified by a unique `id` representing the initiation timestamp.

## Configuration Parameters

Training the decoders necessitates the management of various parameters, facilitated by a `toml` configuration file. This file outlines the training dataset (referring to the `annotation.csv` file with image paths and corresponding labels), among other settings. It also allows the specification of evaluation datasets during or post-training through the `eval.datasets` list. A comprehensive breakdown of parameters, along with default settings, can be found in the comment of the [`default_decoder_config.toml`](default_decoder_config.toml) itself. You can examine example implements of both regression and classification tasks in the [example folder](./examples/). When you train or eval your model providing your own `toml` file, any parameter that is not included in your toml file will default to the parameter specified in the [`default_decoder_config.toml`](./default_decoder_config.toml)


## Examples
We provide two examples, one with a regression task, and one with a classification task. In both cases, we don't provide the trained network, but only the scripts to train and test the networks yourself. This can be used as a template for further testing. 
In both examples we generate a dataset in the `examples/data/` folder. The `eval.toml` and `train.toml` file will then point to the corresponding dataset, and in particular to the dataset's annotation file (e.g. `examples/data/shape_and_object_recognition/embedded_figures/annotation.csv`). Run `train.py` first and then `eval.py`. This is just an illustrative example with few samples: for real life experiments we suggest using a much more sizeable dataset for training.

- ### Notes about Classification ### 
Whereas for classification is quite common to exploit the `PyTorch` `ImageFolder` class, here we don't use it, relying again on the `annotation.csv` file instead to point to image paths. This is done in order to be agnostic to the folder-structure and to be consistent with the rest of the codebase (which also uses the `annotation.csv` file). 


## Training Loop Info
Our training suite is designed to be comprehensive, thus offering a plethora of parameters in the `toml` file and delivering detailed feedback throughout the training process. Here's a closer look at the aspects of the training loop:


#### Logs
For classification, we print the following metrics during the training loop: `ema_loss`, `ema_acc`, `ema_acc_0`, `ema_acc_1`, ..., `ema_acc_5`. That is, a part from providing the loss, we provide the overall accuracy across all decoders, and the accuracy for each individual decoder. `ema` stands for [Exponential Moving Average](https://en.wikipedia.org/wiki/Moving_average) (to smooth across batches).
The output saved in the `csv` file are similarly named, without the `ema`, indicating that the values are simply the overall accuracy of the whole training set.

Similarly for regression, we compute the following metrics: 
`ema_loss`, `ema_rmse`, `ema_rmse_0`, ... `ema_rmse_5`. In this case `rmse` stands for [Root Mean Square Error](https://en.wikipedia.org/wiki/Root-mean-square_deviation). As before, this is computed for each individual decoder (`rmse_x`) and across all decoders.  

#### Interrupting Training
Interrupting training with `CTRL+C` should actually end the training gracefully: the test dataset are computed one final time, the `csv` files are written in the appropriate folder, and the PyTorch model is also saved. In any case, a PyTorch `checkpoint` is saved every epoch for both the model and the optimizer, so that training can be resumed using exactly the same state. 

### Monitoring with Neptune.ai
[Neptune.ai](www.neptune.ai) is a tool for logging experiments on the web. You don't need to use Neptune, but my training loop codebase is well integrated with it: it will save sample images and log training/testing charts in the Neptune page.
If you have an account, set `--neptune_proj_name` to your project name (which must be created through the UI). Also follow the instruction to install the neptune-client and setup the API. Using neptune is optional.  

## Decoder Placement 
```python
output_first_decoder = self.decoders[0](x).squeeze()

x = self.net.conv1(x)
x = self.net.bn1(x)
x = self.net.relu(x)
x = self.net.maxpool(x)

x = self.net.layer1(x)
output_second_decoder = self.decoders[1](x)

x = self.net.layer2(x)
output_third_decoder = self.decoders[2](x)

x = self.net.layer3(x)
output_fourth_decoder = self.decoders[3](x)

x = self.net.layer4(x)
output_fifth_decoder = self.decoders[4](x)

x = self.net.avgpool(x)
x = torch.flatten(x, 1)
output_sixth_decoder = self.decoders[5](x)
``` 

